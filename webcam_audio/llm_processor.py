# llm_processor.py
import os
import sys
import io
import asyncio
from transformers import AutoTokenizer
from optimum.intel.openvino import OVModelForCausalLM
from transformers.generation.streamers import TextStreamer

# edge-tts ìŒì„± ì¶œë ¥
import edge_tts
import playsound

class LLMProcessor:
    def __init__(self, model_name="OpenVINO/Qwen2.5-1.5B-Instruct-fp16-ov"):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = OVModelForCausalLM.from_pretrained(self.model_name)
        self.system_messages = [{
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ITíšŒì‚¬ì—ì„œ ì‚¬ëŒì„ ë½‘ê¸°ìœ„í•´ ê³ ìš©ëœ ë©´ì ‘ê´€ ì…ë‹ˆë‹¤. ì‹œì‘ë˜ë©´ ì‚¬ìš©ìê°€ ë§í•˜ëŠ” 1ë¶„ ìê¸°ì†Œê°œë¥¼ ë“£ê³  íŒë‹¨í•´ì„œ ì—­ëŸ‰ê³¼ ì§ë¬´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•˜ë‚˜ì”© í•˜ì„¸ìš”"  
        }]

    def get_response(self, user_input: str) -> str:
        self.system_messages.append({
            "role": "user",
            "content": user_input
        })

        # Qwen2.5 ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        chat_prompt = self.tokenizer.apply_chat_template(
            self.system_messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(chat_prompt, return_tensors="pt", truncation=True, max_length=1024)

        # ìŠ¤íŠ¸ë¦¬ë¨¸ ì¶œë ¥ ìº¡ì²˜ ì„¤ì •
        old_stdout = sys.stdout
        redirected_output = io.StringIO()
        sys.stdout = redirected_output

        try:
            streamer = TextStreamer(self.tokenizer, skip_prompt=True)

            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                streamer=streamer
            )

            # ìŠ¤íŠ¸ë¦¬ë¨¸ë¡œ ì¶œë ¥ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_output_text = redirected_output.getvalue()

            # ëª¨ë¸ ì‘ë‹µì—ì„œ 'ì±—ë´‡:' ì´í›„ë§Œ ì¶”ì¶œ
            response = full_output_text.strip().split("ì±—ë´‡:")[-1].strip()

            # system_messagesì— ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
            self.system_messages.append({
                "role": "assistant",
                "content": response
            })

            return response

        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        finally:
            sys.stdout = old_stdout


    ### ğŸ—£ï¸ edge-tts ê¸°ë°˜ ì‹¤ì‹œê°„ TTS í•¨ìˆ˜
    async def speak_text(text: str):
        filename = f"./tmp.mp3"

        # ìŒì„± íŒŒì¼ ì €ì¥
        tts = edge_tts.Communicate(text, voice="ko-KR-SunHiNeural")
        await tts.save(filename)

        # ì €ì¥ëœ ìŒì„± ì¬ìƒ
        playsound.playsound(filename)

        # ì¬ìƒ í›„ íŒŒì¼ ì‚­ì œ
        os.remove(filename)
    
    def run(self):
        print("AI ë©´ì ‘ê´€ì˜ ë©´ì ‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚˜ê°€ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")
        welcome_message = "ë¨¼ì € 1ë¶„ìê¸°ì†Œê°œë¥¼ í†µí•´ ìê¸°ì˜ ê²½í—˜ê³¼ ì§ë¬´ì—­ëŸ‰ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"

        
        
        while True:
            user_input = input("\nì‚¬ìš©ì: ")
            if user_input.strip().lower() == "exit":
                print("ì±„íŒ… ì¢…ë£Œ")
                break

            response = self.get_response(user_input)
            print(f"ë©´ì ‘ê´€: {response}")

            # ìŒì„± ì¶œë ¥ (ë¹„ë™ê¸° ì‹¤í–‰)
            asyncio.run(self.speak_text(response))



        
    


### âœ… ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    llm = LLMProcessor()

    print("ë©´ì ‘ê´€ LLMì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚˜ê°€ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")

    while True:
        user_input = input("\nì‚¬ìš©ì: ")
        if user_input.strip().lower() == "exit":
            print("ì±„íŒ… ì¢…ë£Œ")
            break

        response = llm.get_response(user_input)
        print(f"ë©´ì ‘ê´€: {response}")

        # ìŒì„± ì¶œë ¥ (ë¹„ë™ê¸° ì‹¤í–‰)
        asyncio.run(speak_text(response))
